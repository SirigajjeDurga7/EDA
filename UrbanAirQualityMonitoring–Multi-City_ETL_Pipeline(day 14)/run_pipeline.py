# run_pipeline.py

from extract import extract_all
from transform import transform_pipeline
from load import load_to_supabase, create_table_if_not_exists
from etl_analysis import etl_analysis_pipeline
from pathlib import Path


def run_pipeline():
    print("\nğŸš€ Starting FULL ETL + Analysis Pipeline...\n")

    # 1ï¸âƒ£ Extract
    print("ğŸ“¥ STEP 1: Extracting data...")
    extract_all()

    # 2ï¸âƒ£ Transform
    print("\nğŸ”„ STEP 2: Transforming data...")
    df = transform_pipeline()

    # 3ï¸âƒ£ Load
    print("\nğŸ—„ï¸ STEP 3: Loading data into Supabase...")

    # Ensure table exists in Supabase
    create_table_if_not_exists()

    staged_file = Path("data/staged/air_quality_transformed.csv")
    load_to_supabase(str(staged_file))

    # 4ï¸âƒ£ Analysis
    print("\nğŸ“Š STEP 4: Running Analysis...")
    etl_analysis_pipeline()

    print("\nğŸ‰ ETL + Analysis Pipeline Completed Successfully!\n")


if __name__ == "__main__":
    run_pipeline()
